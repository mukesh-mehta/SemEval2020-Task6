{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField, BucketIterator \n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "import spacy \n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import os\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_devices = ','.join([str(id) for id in range(torch.cuda.device_count())])\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(1)\n",
    "print(device)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(\n",
    "    lower=\"True\",\n",
    "    tokenize=tokenizer_en,\n",
    "    sequential=True,\n",
    "#     fix_length=500,\n",
    "    include_lengths=True\n",
    ")\n",
    "\n",
    "LABEL = LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train data is 25000\n",
      "Total test data is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = IMDB.splits(\n",
    "    TEXT, LABEL\n",
    ")\n",
    "# train_data, validation_data = train_data.split(random_state=random.seed(1))\n",
    "\n",
    "print(\"Total train data is {}\".format(len(train_data)))\n",
    "# print(\"Total validation data is {}\".format(len(validation_data)))\n",
    "print(\"Total test data is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31289, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, min_freq=5, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train_data,)\n",
    "len(TEXT.vocab), len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['zentropa', 'has', 'much', 'in', 'common', 'with', 'the', 'third', 'man', ',', 'another', 'noir', '-', 'like', 'film', 'set', 'among', 'the', 'rubble', 'of', 'postwar', 'europe', '.', 'like', 'ttm', ',', 'there', 'is', 'much', 'inventive', 'camera', 'work', '.', 'there', 'is', 'an', 'innocent', 'american', 'who', 'gets', 'emotionally', 'involved', 'with', 'a', 'woman', 'he', 'does', \"n't\", 'really', 'understand', ',', 'and', 'whose', 'naivety', 'is', 'all', 'the', 'more', 'striking', 'in', 'contrast', 'with', 'the', 'natives.<br', '/><br', '/>but', 'i', \"'d\", 'have', 'to', 'say', 'that', 'the', 'third', 'man', 'has', 'a', 'more', 'well', '-', 'crafted', 'storyline', '.', 'zentropa', 'is', 'a', 'bit', 'disjointed', 'in', 'this', 'respect', '.', 'perhaps', 'this', 'is', 'intentional', ':', 'it', 'is', 'presented', 'as', 'a', 'dream', '/', 'nightmare', ',', 'and', 'making', 'it', 'too', 'coherent', 'would', 'spoil', 'the', 'effect', '.', '<', 'br', '/><br', '/>this', 'movie', 'is', 'unrelentingly', 'grim--\"noir', '\"', 'in', 'more', 'than', 'one', 'sense', ';', 'one', 'never', 'sees', 'the', 'sun', 'shine', '.', 'grim', ',', 'but', 'intriguing', ',', 'and', 'frightening', '.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782 782\n"
     ]
    }
   ],
   "source": [
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    device=device,\n",
    "    batch_sizes=(32, 32),\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "#     num_workers=4\n",
    "    \n",
    ")\n",
    "print(len(train_iterator), len(test_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_iterator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  300,    65,   263,  ...,    57,    12, 16958],\n",
       "         [   82,    12,   181,  ...,  6387,   158,    17],\n",
       "         [ 1108,   101,  2384,  ...,    16,    92,     0],\n",
       "         ...,\n",
       "         [  565, 21070,    11,  ...,   146,    30,   122],\n",
       "         [ 1256,   334,     0,  ...,   146,     4,     4],\n",
       "         [    4,  3914,     4,  ...,     1,     1,     1]], device='cuda:1'),\n",
       " torch.Size([32]),\n",
       " torch.Size([32]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text[0], batch.text[1].shape, batch.label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMoji(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_state_size, num_layers,\n",
    "                 output_dim, pad_idx, dropout=0.5, bidirectional=True):\n",
    "\n",
    "        super(DeepMoji, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.num_layers=num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        self.bilstm_one = nn.LSTM(\n",
    "            input_size=self.embedding_dim, hidden_size=self.hidden_state_size,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        # do somethingg about layer two input\n",
    "        \n",
    "        self.bilstm_two = nn.LSTM(\n",
    "            input_size=2 * self.hidden_state_size, hidden_size=self.hidden_state_size,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.attn_layer = AttentionLayer(self.hidden_state_size, self.num_layers, self.embedding_dim)\n",
    "\n",
    "        self.output_layer = nn.Linear(self.hidden_state_size * 2 * self.num_layers, self.output_dim)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.pad_idx).permute(1, 0)\n",
    "        # [batch_size, src_sent_len]\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, inp, src_len):\n",
    "        \n",
    "        # inp = [sent_length, batch_size]\n",
    "        # src_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout_layer(self.embedding(inp))\n",
    "        # embedded = [sent_length, batch_size, embedding_dim]\n",
    "        \n",
    "        embedded_packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
    "        \n",
    "        bilstm_out_1_packed, (_, _) = self.bilstm_one(embedded_packed)\n",
    "        # bilstm_out_1 = [seq_len, batch_size, 2 * hidden_state_size] \n",
    "        \n",
    "        bilstm_out_2_packed, (_, _) = self.bilstm_two(bilstm_out_1_packed) \n",
    "        # bilstm_out_1 = [seq_len, batch_size, 2 * hidden_state_size]\n",
    "        \n",
    "        bilstm_out_1, _ = nn.utils.rnn.pad_packed_sequence(bilstm_out_1_packed)\n",
    "        bilstm_out_2, _ = nn.utils.rnn.pad_packed_sequence(bilstm_out_2_packed)\n",
    "                \n",
    "        bilstm_stacked = torch.cat((bilstm_out_1, bilstm_out_2), dim=2)\n",
    "        # bilstm_stacked = [seq_len, batch_size, 4 * hidden_state_size]\n",
    "                \n",
    "        mask = self.create_mask(inp)\n",
    "        \n",
    "        attn_weights = self.attn_layer(embedded, bilstm_stacked, mask).unsqueeze(1)\n",
    "        # attn_weights = [batch_size, 1, seq_len]\n",
    "        \n",
    "        attended_hidden_representation = torch.bmm(attn_weights, bilstm_stacked.permute(1, 0, 2)).squeeze(1)\n",
    "        # attended_hidden_representation = [batch_size, 4 * hidden_state]\n",
    "        \n",
    "        # print(\"attended shape == {}\".format(attended_hidden_representation.shape))\n",
    "\n",
    "        outputs = self.output_layer(attended_hidden_representation)\n",
    "        # outputs = [batch_size, output_dim]\n",
    "        # print(\"Output shape == {}\".format(outputs.shape))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_state_size, num_layers, embedding_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "#         self.attn = nn.Linear((self.num_layers * 2 * self.hidden_state_size) + self.embedding_dim, self.hidden_state_size)\n",
    "        \n",
    "        self.attn = nn.Parameter(torch.rand((self.num_layers * 2 * self.hidden_state_size) + self.embedding_dim))\n",
    "        \n",
    "        \n",
    "    def forward(self, embedded, lstm_outputs, mask):\n",
    "        # embedded = [src_sent_len, batch_size, embedding_dim]\n",
    "        # lstm_outputs = [src_sent_len, batch_size, enc_hidden_state_size*2*2]\n",
    "        \n",
    "        batch_size = embedded.shape[1]\n",
    "        # calculating energies\n",
    "        \n",
    "        embed_concat = torch.cat((embedded, lstm_outputs), dim=2)\n",
    "        # embed_concat = [src_sent_len, batch_size, embedding_dim + hidden_state_size*4]\n",
    "        \n",
    "        attn = self.attn.repeat(batch_size, 1).unsqueeze(2)\n",
    "        # attn = [batch_size, 4*hidden_state + embedding_dim, 1]\n",
    "        \n",
    "        attention = torch.bmm(embed_concat.permute(1,0,2), attn).squeeze(2)\n",
    "        # attention = [batch_size, seq_len]\n",
    "        \n",
    "        attn_weights = attention.masked_fill(mask == 0, -1e10)\n",
    "        # attn_weights = [batch_size, seq_len]\n",
    "        \n",
    "        attn_weights = F.softmax(attention, dim=1)\n",
    "        # attn_weights = [batch_size, seq_len]\n",
    "        \n",
    "\n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    for batch in tqdm(iterator):\n",
    "        src = batch.text[0]\n",
    "        src_len = batch.text[1]\n",
    "        trg = batch.label\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_outputs = model(src, src_len).squeeze(1)\n",
    "#         print(model_outputs.shape)\n",
    "        # [batch_size, output_dim]\n",
    "        \n",
    "        loss = criterion(model_outputs, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_acc += binary_accuracy(model_outputs, trg)\n",
    "        epoch_loss += loss.item()\n",
    "#         break\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    for batch in tqdm(iterator):\n",
    "        src = batch.text[0]\n",
    "        src_len = batch.text[1]\n",
    "        trg = batch.label\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_outputs = model(src, src_len).squeeze(1)\n",
    "        # [batch_size, output_dim]\n",
    "        \n",
    "        loss = criterion(model_outputs, trg)  \n",
    "        \n",
    "        epoch_acc += binary_accuracy(model_outputs, trg)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator),  epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "        \n",
    "def binary_accuracy(model_outputs, truth):\n",
    "    # model_outputs = [batch_size]\n",
    "    # truth = [batch]\n",
    "    \n",
    "    predictions = torch.round(torch.sigmoid(model_outputs))\n",
    "    correct = (predictions == truth).float()\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepMoji(\n",
    "    vocab_size=len(TEXT.vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_state_size=512,\n",
    "    num_layers=2,\n",
    "    output_dim=1,\n",
    "    dropout=0.5,\n",
    "    bidirectional=True,\n",
    "    pad_idx=TEXT.vocab.stoi[\"<pad>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepMoji(\n",
       "  (embedding): Embedding(31289, 100)\n",
       "  (bilstm_one): LSTM(100, 512, bidirectional=True)\n",
       "  (bilstm_two): LSTM(1024, 512, bidirectional=True)\n",
       "  (attn_layer): AttentionLayer()\n",
       "  (output_layer): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  (dropout_layer): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 11,947,689 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:14<00:00,  5.82it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.14it/s]\n",
      "  0%|          | 1/782 [00:00<01:26,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 57s\n",
      "\tTrain Loss: 0.693 | Train Acc: 54.12%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 53.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:12<00:00,  5.90it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.08it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 2m 56s\n",
      "\tTrain Loss: 0.679 | Train Acc: 56.68%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 55.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:13<00:00,  4.67it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.08it/s]\n",
      "  0%|          | 1/782 [00:00<01:58,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 2m 57s\n",
      "\tTrain Loss: 0.642 | Train Acc: 61.41%\n",
      "\t Val. Loss: 0.491 |  Val. Acc: 77.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:12<00:00,  4.95it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.11it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 2m 55s\n",
      "\tTrain Loss: 0.417 | Train Acc: 81.24%\n",
      "\t Val. Loss: 0.326 |  Val. Acc: 86.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:11<00:00,  5.96it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.08it/s]\n",
      "  0%|          | 1/782 [00:00<02:03,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 2m 54s\n",
      "\tTrain Loss: 0.333 | Train Acc: 85.90%\n",
      "\t Val. Loss: 0.305 |  Val. Acc: 87.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:14<00:00,  5.82it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.08it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 2m 57s\n",
      "\tTrain Loss: 0.276 | Train Acc: 88.71%\n",
      "\t Val. Loss: 0.282 |  Val. Acc: 88.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:14<00:00,  5.24it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.07it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 2m 57s\n",
      "\tTrain Loss: 0.245 | Train Acc: 90.01%\n",
      "\t Val. Loss: 0.268 |  Val. Acc: 89.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:12<00:00,  4.66it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.11it/s]\n",
      "  0%|          | 1/782 [00:00<01:59,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 2m 55s\n",
      "\tTrain Loss: 0.222 | Train Acc: 91.19%\n",
      "\t Val. Loss: 0.271 |  Val. Acc: 89.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:12<00:00,  5.45it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.13it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 2m 55s\n",
      "\tTrain Loss: 0.196 | Train Acc: 92.43%\n",
      "\t Val. Loss: 0.260 |  Val. Acc: 89.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:11<00:00,  4.83it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.09it/s]\n",
      "  0%|          | 1/782 [00:00<01:58,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 2m 54s\n",
      "\tTrain Loss: 0.179 | Train Acc: 93.13%\n",
      "\t Val. Loss: 0.256 |  Val. Acc: 89.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:13<00:00,  5.66it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.06it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Time: 2m 57s\n",
      "\tTrain Loss: 0.166 | Train Acc: 93.52%\n",
      "\t Val. Loss: 0.251 |  Val. Acc: 90.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:15<00:00,  4.13it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.06it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Time: 2m 58s\n",
      "\tTrain Loss: 0.143 | Train Acc: 94.73%\n",
      "\t Val. Loss: 0.263 |  Val. Acc: 90.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:14<00:00,  6.11it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.12it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Time: 2m 57s\n",
      "\tTrain Loss: 0.133 | Train Acc: 95.08%\n",
      "\t Val. Loss: 0.312 |  Val. Acc: 89.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:14<00:00,  5.82it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.11it/s]\n",
      "  0%|          | 1/782 [00:00<02:01,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Time: 2m 57s\n",
      "\tTrain Loss: 0.123 | Train Acc: 95.36%\n",
      "\t Val. Loss: 0.301 |  Val. Acc: 90.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:14<00:00,  3.97it/s]\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.17it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Time: 2m 57s\n",
      "\tTrain Loss: 0.107 | Train Acc: 96.08%\n",
      "\t Val. Loss: 0.329 |  Val. Acc: 90.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 81/782 [00:14<02:01,  5.75it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-18548fb22a00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     train_loss, train_acc = train(model, train_iterator, optimizer, criterion,\n\u001b[0;32m---> 11\u001b[0;31m                        CLIP)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-85ab49e2ccaf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/users-workspace/kuldeep.singh/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/users-workspace/kuldeep.singh/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 25\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion,\n",
    "                       CLIP)\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'deepmoji-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('deepmoji-model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 8/782 [00:00<00:10, 74.14it/s]\u001b[A\n",
      "  2%|▏         | 16/782 [00:00<00:10, 75.80it/s]\u001b[A\n",
      "  3%|▎         | 24/782 [00:00<00:09, 75.83it/s]\u001b[A\n",
      "  4%|▍         | 31/782 [00:00<00:10, 73.83it/s]\u001b[A\n",
      "  5%|▍         | 38/782 [00:00<00:10, 70.78it/s]\u001b[A\n",
      "  6%|▌         | 45/782 [00:00<00:10, 67.63it/s]\u001b[A\n",
      "  7%|▋         | 51/782 [00:00<00:11, 64.10it/s]\u001b[A\n",
      "  7%|▋         | 57/782 [00:00<00:11, 60.74it/s]\u001b[A\n",
      "  8%|▊         | 63/782 [00:00<00:12, 57.85it/s]\u001b[A\n",
      "  9%|▉         | 69/782 [00:01<00:12, 54.98it/s]\u001b[A\n",
      " 10%|▉         | 75/782 [00:01<00:13, 52.32it/s]\u001b[A\n",
      " 10%|█         | 81/782 [00:01<00:14, 49.98it/s]\u001b[A\n",
      " 11%|█         | 86/782 [00:01<00:14, 47.62it/s]\u001b[A\n",
      " 12%|█▏        | 91/782 [00:01<00:15, 45.65it/s]\u001b[A\n",
      " 12%|█▏        | 96/782 [00:01<00:15, 43.94it/s]\u001b[A\n",
      " 13%|█▎        | 101/782 [00:01<00:16, 42.54it/s]\u001b[A\n",
      " 14%|█▎        | 106/782 [00:01<00:16, 41.39it/s]\u001b[A\n",
      " 14%|█▍        | 111/782 [00:02<00:16, 40.64it/s]\u001b[A\n",
      " 15%|█▍        | 116/782 [00:02<00:16, 39.65it/s]\u001b[A\n",
      " 15%|█▌        | 120/782 [00:02<00:17, 38.88it/s]\u001b[A\n",
      " 16%|█▌        | 124/782 [00:02<00:17, 38.20it/s]\u001b[A\n",
      " 16%|█▋        | 128/782 [00:02<00:17, 37.55it/s]\u001b[A\n",
      " 17%|█▋        | 132/782 [00:02<00:17, 37.12it/s]\u001b[A\n",
      " 17%|█▋        | 136/782 [00:02<00:17, 36.86it/s]\u001b[A\n",
      " 18%|█▊        | 140/782 [00:02<00:17, 36.43it/s]\u001b[A\n",
      " 18%|█▊        | 144/782 [00:03<00:17, 36.09it/s]\u001b[A\n",
      " 19%|█▉        | 148/782 [00:03<00:17, 35.74it/s]\u001b[A\n",
      " 19%|█▉        | 152/782 [00:03<00:17, 35.45it/s]\u001b[A\n",
      " 20%|█▉        | 156/782 [00:03<00:17, 35.14it/s]\u001b[A\n",
      " 20%|██        | 160/782 [00:03<00:17, 34.96it/s]\u001b[A\n",
      " 21%|██        | 164/782 [00:03<00:17, 34.59it/s]\u001b[A\n",
      " 21%|██▏       | 168/782 [00:03<00:17, 34.15it/s]\u001b[A\n",
      " 22%|██▏       | 172/782 [00:03<00:17, 33.95it/s]\u001b[A\n",
      " 23%|██▎       | 176/782 [00:03<00:17, 33.81it/s]\u001b[A\n",
      " 23%|██▎       | 180/782 [00:04<00:17, 33.92it/s]\u001b[A\n",
      " 24%|██▎       | 184/782 [00:04<00:17, 33.66it/s]\u001b[A\n",
      " 24%|██▍       | 188/782 [00:04<00:17, 33.56it/s]\u001b[A\n",
      " 25%|██▍       | 192/782 [00:04<00:17, 33.30it/s]\u001b[A\n",
      " 25%|██▌       | 196/782 [00:04<00:17, 33.23it/s]\u001b[A\n",
      " 26%|██▌       | 200/782 [00:04<00:17, 33.09it/s]\u001b[A\n",
      " 26%|██▌       | 204/782 [00:04<00:17, 32.92it/s]\u001b[A\n",
      " 27%|██▋       | 208/782 [00:04<00:17, 32.67it/s]\u001b[A\n",
      " 27%|██▋       | 212/782 [00:05<00:17, 32.36it/s]\u001b[A\n",
      " 28%|██▊       | 216/782 [00:05<00:17, 32.22it/s]\u001b[A\n",
      " 28%|██▊       | 220/782 [00:05<00:17, 32.02it/s]\u001b[A\n",
      " 29%|██▊       | 224/782 [00:05<00:17, 31.75it/s]\u001b[A\n",
      " 29%|██▉       | 228/782 [00:05<00:17, 31.83it/s]\u001b[A\n",
      " 30%|██▉       | 232/782 [00:05<00:17, 31.62it/s]\u001b[A\n",
      " 30%|███       | 236/782 [00:05<00:17, 31.47it/s]\u001b[A\n",
      " 31%|███       | 240/782 [00:05<00:17, 31.13it/s]\u001b[A\n",
      " 31%|███       | 244/782 [00:06<00:17, 31.05it/s]\u001b[A\n",
      " 32%|███▏      | 248/782 [00:06<00:17, 30.91it/s]\u001b[A\n",
      " 32%|███▏      | 252/782 [00:06<00:17, 30.74it/s]\u001b[A\n",
      " 33%|███▎      | 256/782 [00:06<00:17, 30.56it/s]\u001b[A\n",
      " 33%|███▎      | 260/782 [00:06<00:17, 30.34it/s]\u001b[A\n",
      " 34%|███▍      | 264/782 [00:06<00:17, 30.11it/s]\u001b[A\n",
      " 34%|███▍      | 268/782 [00:06<00:17, 29.90it/s]\u001b[A\n",
      " 35%|███▍      | 271/782 [00:06<00:17, 29.86it/s]\u001b[A\n",
      " 35%|███▌      | 274/782 [00:07<00:17, 29.61it/s]\u001b[A\n",
      " 35%|███▌      | 277/782 [00:07<00:17, 29.44it/s]\u001b[A\n",
      " 36%|███▌      | 280/782 [00:07<00:17, 29.45it/s]\u001b[A\n",
      " 36%|███▌      | 283/782 [00:07<00:17, 29.15it/s]\u001b[A\n",
      " 37%|███▋      | 286/782 [00:07<00:17, 28.99it/s]\u001b[A\n",
      " 37%|███▋      | 289/782 [00:07<00:17, 28.72it/s]\u001b[A\n",
      " 37%|███▋      | 292/782 [00:07<00:17, 28.54it/s]\u001b[A\n",
      " 38%|███▊      | 295/782 [00:07<00:17, 28.41it/s]\u001b[A\n",
      " 38%|███▊      | 298/782 [00:07<00:17, 28.31it/s]\u001b[A\n",
      " 38%|███▊      | 301/782 [00:08<00:17, 28.14it/s]\u001b[A\n",
      " 39%|███▉      | 304/782 [00:08<00:17, 27.99it/s]\u001b[A\n",
      " 39%|███▉      | 307/782 [00:08<00:17, 27.84it/s]\u001b[A\n",
      " 40%|███▉      | 310/782 [00:08<00:17, 27.74it/s]\u001b[A\n",
      " 40%|████      | 313/782 [00:08<00:17, 27.55it/s]\u001b[A\n",
      " 40%|████      | 316/782 [00:08<00:16, 27.46it/s]\u001b[A\n",
      " 41%|████      | 319/782 [00:08<00:16, 27.33it/s]\u001b[A\n",
      " 41%|████      | 322/782 [00:08<00:16, 27.12it/s]\u001b[A\n",
      " 42%|████▏     | 325/782 [00:08<00:16, 26.89it/s]\u001b[A\n",
      " 42%|████▏     | 328/782 [00:09<00:17, 26.63it/s]\u001b[A\n",
      " 42%|████▏     | 331/782 [00:09<00:17, 26.50it/s]\u001b[A\n",
      " 43%|████▎     | 334/782 [00:09<00:16, 26.42it/s]\u001b[A\n",
      " 43%|████▎     | 337/782 [00:09<00:16, 26.18it/s]\u001b[A\n",
      " 43%|████▎     | 340/782 [00:09<00:16, 26.15it/s]\u001b[A\n",
      " 44%|████▍     | 343/782 [00:09<00:16, 25.92it/s]\u001b[A\n",
      " 44%|████▍     | 346/782 [00:09<00:16, 25.77it/s]\u001b[A\n",
      " 45%|████▍     | 349/782 [00:09<00:16, 25.65it/s]\u001b[A\n",
      " 45%|████▌     | 352/782 [00:09<00:16, 25.51it/s]\u001b[A\n",
      " 45%|████▌     | 355/782 [00:10<00:16, 25.40it/s]\u001b[A\n",
      " 46%|████▌     | 358/782 [00:10<00:16, 25.49it/s]\u001b[A\n",
      " 46%|████▌     | 361/782 [00:10<00:16, 25.26it/s]\u001b[A\n",
      " 47%|████▋     | 364/782 [00:10<00:16, 25.07it/s]\u001b[A\n",
      " 47%|████▋     | 367/782 [00:10<00:16, 24.92it/s]\u001b[A\n",
      " 47%|████▋     | 370/782 [00:10<00:16, 24.79it/s]\u001b[A\n",
      " 48%|████▊     | 373/782 [00:10<00:16, 24.73it/s]\u001b[A\n",
      " 48%|████▊     | 376/782 [00:10<00:16, 24.59it/s]\u001b[A\n",
      " 48%|████▊     | 379/782 [00:11<00:16, 24.43it/s]\u001b[A\n",
      " 49%|████▉     | 382/782 [00:11<00:16, 24.26it/s]\u001b[A\n",
      " 49%|████▉     | 385/782 [00:11<00:16, 24.13it/s]\u001b[A\n",
      " 50%|████▉     | 388/782 [00:11<00:16, 23.96it/s]\u001b[A\n",
      " 50%|█████     | 391/782 [00:11<00:16, 23.80it/s]\u001b[A\n",
      " 50%|█████     | 394/782 [00:11<00:16, 23.69it/s]\u001b[A\n",
      " 51%|█████     | 397/782 [00:11<00:16, 23.56it/s]\u001b[A\n",
      " 51%|█████     | 400/782 [00:11<00:16, 23.41it/s]\u001b[A\n",
      " 52%|█████▏    | 403/782 [00:12<00:16, 23.32it/s]\u001b[A\n",
      " 52%|█████▏    | 406/782 [00:12<00:16, 23.27it/s]\u001b[A\n",
      " 52%|█████▏    | 409/782 [00:12<00:16, 23.00it/s]\u001b[A\n",
      " 53%|█████▎    | 412/782 [00:12<00:16, 22.93it/s]\u001b[A\n",
      " 53%|█████▎    | 415/782 [00:12<00:16, 22.84it/s]\u001b[A\n",
      " 53%|█████▎    | 418/782 [00:12<00:15, 22.78it/s]\u001b[A\n",
      " 54%|█████▍    | 421/782 [00:12<00:15, 22.65it/s]\u001b[A\n",
      " 54%|█████▍    | 424/782 [00:12<00:15, 22.55it/s]\u001b[A\n",
      " 55%|█████▍    | 427/782 [00:13<00:15, 22.37it/s]\u001b[A\n",
      " 55%|█████▍    | 430/782 [00:13<00:15, 22.36it/s]\u001b[A\n",
      " 55%|█████▌    | 433/782 [00:13<00:15, 22.30it/s]\u001b[A\n",
      " 56%|█████▌    | 436/782 [00:13<00:15, 22.13it/s]\u001b[A\n",
      " 56%|█████▌    | 439/782 [00:13<00:15, 22.01it/s]\u001b[A\n",
      " 57%|█████▋    | 442/782 [00:13<00:15, 21.90it/s]\u001b[A\n",
      " 57%|█████▋    | 445/782 [00:13<00:15, 21.72it/s]\u001b[A\n",
      " 57%|█████▋    | 448/782 [00:14<00:15, 21.61it/s]\u001b[A\n",
      " 58%|█████▊    | 451/782 [00:14<00:15, 21.55it/s]\u001b[A\n",
      " 58%|█████▊    | 454/782 [00:14<00:15, 21.41it/s]\u001b[A\n",
      " 58%|█████▊    | 457/782 [00:14<00:15, 21.25it/s]\u001b[A\n",
      " 59%|█████▉    | 460/782 [00:14<00:15, 21.06it/s]\u001b[A\n",
      " 59%|█████▉    | 463/782 [00:14<00:15, 20.88it/s]\u001b[A\n",
      " 60%|█████▉    | 466/782 [00:14<00:15, 20.79it/s]\u001b[A\n",
      " 60%|█████▉    | 469/782 [00:15<00:15, 20.61it/s]\u001b[A\n",
      " 60%|██████    | 472/782 [00:15<00:15, 20.54it/s]\u001b[A\n",
      " 61%|██████    | 475/782 [00:15<00:15, 20.41it/s]\u001b[A\n",
      " 61%|██████    | 478/782 [00:15<00:15, 20.24it/s]\u001b[A\n",
      " 62%|██████▏   | 481/782 [00:15<00:14, 20.17it/s]\u001b[A\n",
      " 62%|██████▏   | 484/782 [00:15<00:14, 20.05it/s]\u001b[A\n",
      " 62%|██████▏   | 487/782 [00:16<00:14, 19.87it/s]\u001b[A\n",
      " 63%|██████▎   | 489/782 [00:16<00:14, 19.78it/s]\u001b[A\n",
      " 63%|██████▎   | 491/782 [00:16<00:14, 19.64it/s]\u001b[A\n",
      " 63%|██████▎   | 493/782 [00:16<00:14, 19.47it/s]\u001b[A\n",
      " 63%|██████▎   | 495/782 [00:16<00:14, 19.39it/s]\u001b[A\n",
      " 64%|██████▎   | 497/782 [00:16<00:14, 19.20it/s]\u001b[A\n",
      " 64%|██████▍   | 499/782 [00:16<00:14, 19.04it/s]\u001b[A\n",
      " 64%|██████▍   | 501/782 [00:16<00:14, 18.98it/s]\u001b[A\n",
      " 64%|██████▍   | 503/782 [00:16<00:14, 18.90it/s]\u001b[A\n",
      " 65%|██████▍   | 505/782 [00:16<00:14, 18.86it/s]\u001b[A\n",
      " 65%|██████▍   | 507/782 [00:17<00:14, 18.77it/s]\u001b[A\n",
      " 65%|██████▌   | 509/782 [00:17<00:14, 18.70it/s]\u001b[A\n",
      " 65%|██████▌   | 511/782 [00:17<00:14, 18.59it/s]\u001b[A\n",
      " 66%|██████▌   | 513/782 [00:17<00:14, 18.31it/s]\u001b[A\n",
      " 66%|██████▌   | 515/782 [00:17<00:14, 18.37it/s]\u001b[A\n",
      " 66%|██████▌   | 517/782 [00:17<00:14, 18.24it/s]\u001b[A\n",
      " 66%|██████▋   | 519/782 [00:17<00:14, 18.05it/s]\u001b[A\n",
      " 67%|██████▋   | 521/782 [00:17<00:14, 18.02it/s]\u001b[A\n",
      " 67%|██████▋   | 523/782 [00:17<00:14, 17.96it/s]\u001b[A\n",
      " 67%|██████▋   | 525/782 [00:18<00:14, 17.83it/s]\u001b[A\n",
      " 67%|██████▋   | 527/782 [00:18<00:14, 17.78it/s]\u001b[A\n",
      " 68%|██████▊   | 529/782 [00:18<00:14, 17.67it/s]\u001b[A\n",
      " 68%|██████▊   | 531/782 [00:18<00:14, 17.61it/s]\u001b[A\n",
      " 68%|██████▊   | 533/782 [00:18<00:14, 17.51it/s]\u001b[A\n",
      " 68%|██████▊   | 535/782 [00:18<00:14, 17.38it/s]\u001b[A\n",
      " 69%|██████▊   | 537/782 [00:18<00:14, 17.34it/s]\u001b[A\n",
      " 69%|██████▉   | 539/782 [00:18<00:14, 17.25it/s]\u001b[A\n",
      " 69%|██████▉   | 541/782 [00:18<00:14, 17.18it/s]\u001b[A\n",
      " 69%|██████▉   | 543/782 [00:19<00:13, 17.10it/s]\u001b[A\n",
      " 70%|██████▉   | 545/782 [00:19<00:13, 16.99it/s]\u001b[A\n",
      " 70%|██████▉   | 547/782 [00:19<00:13, 16.82it/s]\u001b[A\n",
      " 70%|███████   | 549/782 [00:19<00:14, 16.63it/s]\u001b[A\n",
      " 70%|███████   | 551/782 [00:19<00:13, 16.56it/s]\u001b[A\n",
      " 71%|███████   | 553/782 [00:19<00:13, 16.41it/s]\u001b[A\n",
      " 71%|███████   | 555/782 [00:19<00:13, 16.33it/s]\u001b[A\n",
      " 71%|███████   | 557/782 [00:19<00:13, 16.30it/s]\u001b[A\n",
      " 71%|███████▏  | 559/782 [00:20<00:13, 16.24it/s]\u001b[A\n",
      " 72%|███████▏  | 561/782 [00:20<00:13, 16.19it/s]\u001b[A\n",
      " 72%|███████▏  | 563/782 [00:20<00:13, 16.09it/s]\u001b[A\n",
      " 72%|███████▏  | 565/782 [00:20<00:13, 16.07it/s]\u001b[A\n",
      " 73%|███████▎  | 567/782 [00:20<00:13, 15.95it/s]\u001b[A\n",
      " 73%|███████▎  | 569/782 [00:20<00:13, 15.78it/s]\u001b[A\n",
      " 73%|███████▎  | 571/782 [00:20<00:13, 15.68it/s]\u001b[A\n",
      " 73%|███████▎  | 573/782 [00:20<00:13, 15.53it/s]\u001b[A\n",
      " 74%|███████▎  | 575/782 [00:21<00:13, 15.55it/s]\u001b[A\n",
      " 74%|███████▍  | 577/782 [00:21<00:13, 15.47it/s]\u001b[A\n",
      " 74%|███████▍  | 579/782 [00:21<00:13, 15.32it/s]\u001b[A\n",
      " 74%|███████▍  | 581/782 [00:21<00:13, 15.29it/s]\u001b[A\n",
      " 75%|███████▍  | 583/782 [00:21<00:13, 15.21it/s]\u001b[A\n",
      " 75%|███████▍  | 585/782 [00:21<00:13, 15.11it/s]\u001b[A\n",
      " 75%|███████▌  | 587/782 [00:21<00:12, 15.00it/s]\u001b[A\n",
      " 75%|███████▌  | 589/782 [00:22<00:12, 14.89it/s]\u001b[A\n",
      " 76%|███████▌  | 591/782 [00:22<00:12, 14.83it/s]\u001b[A\n",
      " 76%|███████▌  | 593/782 [00:22<00:12, 14.76it/s]\u001b[A\n",
      " 76%|███████▌  | 595/782 [00:22<00:12, 14.74it/s]\u001b[A\n",
      " 76%|███████▋  | 597/782 [00:22<00:12, 14.70it/s]\u001b[A\n",
      " 77%|███████▋  | 599/782 [00:22<00:12, 14.51it/s]\u001b[A\n",
      " 77%|███████▋  | 601/782 [00:22<00:12, 14.40it/s]\u001b[A\n",
      " 77%|███████▋  | 603/782 [00:23<00:12, 14.26it/s]\u001b[A\n",
      " 77%|███████▋  | 605/782 [00:23<00:12, 14.16it/s]\u001b[A\n",
      " 78%|███████▊  | 607/782 [00:23<00:12, 14.06it/s]\u001b[A\n",
      " 78%|███████▊  | 609/782 [00:23<00:12, 14.00it/s]\u001b[A\n",
      " 78%|███████▊  | 611/782 [00:23<00:12, 13.89it/s]\u001b[A\n",
      " 78%|███████▊  | 613/782 [00:23<00:12, 13.76it/s]\u001b[A\n",
      " 79%|███████▊  | 615/782 [00:23<00:12, 13.69it/s]\u001b[A\n",
      " 79%|███████▉  | 617/782 [00:24<00:12, 13.69it/s]\u001b[A\n",
      " 79%|███████▉  | 619/782 [00:24<00:11, 13.61it/s]\u001b[A\n",
      " 79%|███████▉  | 621/782 [00:24<00:11, 13.59it/s]\u001b[A\n",
      " 80%|███████▉  | 623/782 [00:24<00:11, 13.43it/s]\u001b[A\n",
      " 80%|███████▉  | 625/782 [00:24<00:11, 13.28it/s]\u001b[A\n",
      " 80%|████████  | 627/782 [00:24<00:11, 13.18it/s]\u001b[A\n",
      " 80%|████████  | 629/782 [00:24<00:11, 13.12it/s]\u001b[A\n",
      " 81%|████████  | 631/782 [00:25<00:11, 13.04it/s]\u001b[A\n",
      " 81%|████████  | 633/782 [00:25<00:11, 12.92it/s]\u001b[A\n",
      " 81%|████████  | 635/782 [00:25<00:11, 12.86it/s]\u001b[A\n",
      " 81%|████████▏ | 637/782 [00:25<00:11, 12.70it/s]\u001b[A\n",
      " 82%|████████▏ | 639/782 [00:25<00:11, 12.64it/s]\u001b[A\n",
      " 82%|████████▏ | 641/782 [00:25<00:11, 12.60it/s]\u001b[A\n",
      " 82%|████████▏ | 643/782 [00:26<00:11, 12.43it/s]\u001b[A\n",
      " 82%|████████▏ | 645/782 [00:26<00:11, 12.38it/s]\u001b[A\n",
      " 83%|████████▎ | 647/782 [00:26<00:10, 12.28it/s]\u001b[A\n",
      " 83%|████████▎ | 649/782 [00:26<00:10, 12.22it/s]\u001b[A\n",
      " 83%|████████▎ | 651/782 [00:26<00:10, 12.10it/s]\u001b[A\n",
      " 84%|████████▎ | 653/782 [00:26<00:10, 12.00it/s]\u001b[A\n",
      " 84%|████████▍ | 655/782 [00:27<00:10, 11.92it/s]\u001b[A\n",
      " 84%|████████▍ | 657/782 [00:27<00:10, 11.80it/s]\u001b[A\n",
      " 84%|████████▍ | 659/782 [00:27<00:10, 11.68it/s]\u001b[A\n",
      " 85%|████████▍ | 661/782 [00:27<00:10, 11.58it/s]\u001b[A\n",
      " 85%|████████▍ | 663/782 [00:27<00:10, 11.46it/s]\u001b[A\n",
      " 85%|████████▌ | 665/782 [00:27<00:10, 11.42it/s]\u001b[A\n",
      " 85%|████████▌ | 667/782 [00:28<00:10, 11.29it/s]\u001b[A\n",
      " 86%|████████▌ | 669/782 [00:28<00:10, 11.21it/s]\u001b[A\n",
      " 86%|████████▌ | 671/782 [00:28<00:10, 11.07it/s]\u001b[A\n",
      " 86%|████████▌ | 673/782 [00:28<00:09, 10.98it/s]\u001b[A\n",
      " 86%|████████▋ | 675/782 [00:28<00:09, 10.91it/s]\u001b[A\n",
      " 87%|████████▋ | 677/782 [00:29<00:09, 10.78it/s]\u001b[A\n",
      " 87%|████████▋ | 679/782 [00:29<00:09, 10.74it/s]\u001b[A\n",
      " 87%|████████▋ | 681/782 [00:29<00:09, 10.69it/s]\u001b[A\n",
      " 87%|████████▋ | 683/782 [00:29<00:09, 10.61it/s]\u001b[A\n",
      " 88%|████████▊ | 685/782 [00:29<00:09, 10.53it/s]\u001b[A\n",
      " 88%|████████▊ | 687/782 [00:30<00:09, 10.45it/s]\u001b[A\n",
      " 88%|████████▊ | 689/782 [00:30<00:09, 10.33it/s]\u001b[A\n",
      " 88%|████████▊ | 691/782 [00:30<00:08, 10.25it/s]\u001b[A\n",
      " 89%|████████▊ | 693/782 [00:30<00:08, 10.17it/s]\u001b[A\n",
      " 89%|████████▉ | 695/782 [00:30<00:08, 10.09it/s]\u001b[A\n",
      " 89%|████████▉ | 697/782 [00:31<00:08,  9.93it/s]\u001b[A\n",
      " 89%|████████▉ | 698/782 [00:31<00:08,  9.77it/s]\u001b[A\n",
      " 89%|████████▉ | 699/782 [00:31<00:08,  9.60it/s]\u001b[A\n",
      " 90%|████████▉ | 700/782 [00:31<00:08,  9.58it/s]\u001b[A\n",
      " 90%|████████▉ | 701/782 [00:31<00:08,  9.50it/s]\u001b[A\n",
      " 90%|████████▉ | 702/782 [00:31<00:08,  9.42it/s]\u001b[A\n",
      " 90%|████████▉ | 703/782 [00:31<00:08,  9.35it/s]\u001b[A\n",
      " 90%|█████████ | 704/782 [00:31<00:08,  9.29it/s]\u001b[A\n",
      " 90%|█████████ | 705/782 [00:31<00:08,  9.23it/s]\u001b[A\n",
      " 90%|█████████ | 706/782 [00:32<00:08,  9.18it/s]\u001b[A\n",
      " 90%|█████████ | 707/782 [00:32<00:08,  9.14it/s]\u001b[A\n",
      " 91%|█████████ | 708/782 [00:32<00:08,  9.06it/s]\u001b[A\n",
      " 91%|█████████ | 709/782 [00:32<00:08,  9.04it/s]\u001b[A\n",
      " 91%|█████████ | 710/782 [00:32<00:08,  8.99it/s]\u001b[A\n",
      " 91%|█████████ | 711/782 [00:32<00:07,  8.95it/s]\u001b[A\n",
      " 91%|█████████ | 712/782 [00:32<00:07,  8.89it/s]\u001b[A\n",
      " 91%|█████████ | 713/782 [00:32<00:07,  8.80it/s]\u001b[A\n",
      " 91%|█████████▏| 714/782 [00:32<00:07,  8.76it/s]\u001b[A\n",
      " 91%|█████████▏| 715/782 [00:33<00:07,  8.78it/s]\u001b[A\n",
      " 92%|█████████▏| 716/782 [00:33<00:07,  8.77it/s]\u001b[A\n",
      " 92%|█████████▏| 717/782 [00:33<00:07,  8.68it/s]\u001b[A\n",
      " 92%|█████████▏| 718/782 [00:33<00:07,  8.58it/s]\u001b[A\n",
      " 92%|█████████▏| 719/782 [00:33<00:07,  8.51it/s]\u001b[A\n",
      " 92%|█████████▏| 720/782 [00:33<00:07,  8.51it/s]\u001b[A\n",
      " 92%|█████████▏| 721/782 [00:33<00:07,  8.45it/s]\u001b[A\n",
      " 92%|█████████▏| 722/782 [00:33<00:07,  8.34it/s]\u001b[A\n",
      " 92%|█████████▏| 723/782 [00:33<00:07,  8.26it/s]\u001b[A\n",
      " 93%|█████████▎| 724/782 [00:34<00:07,  8.21it/s]\u001b[A\n",
      " 93%|█████████▎| 725/782 [00:34<00:06,  8.20it/s]\u001b[A\n",
      " 93%|█████████▎| 726/782 [00:34<00:06,  8.13it/s]\u001b[A\n",
      " 93%|█████████▎| 727/782 [00:34<00:06,  8.08it/s]\u001b[A\n",
      " 93%|█████████▎| 728/782 [00:34<00:06,  8.00it/s]\u001b[A\n",
      " 93%|█████████▎| 729/782 [00:34<00:06,  7.96it/s]\u001b[A\n",
      " 93%|█████████▎| 730/782 [00:34<00:06,  7.90it/s]\u001b[A\n",
      " 93%|█████████▎| 731/782 [00:35<00:06,  7.83it/s]\u001b[A\n",
      " 94%|█████████▎| 732/782 [00:35<00:06,  7.80it/s]\u001b[A\n",
      " 94%|█████████▎| 733/782 [00:35<00:06,  7.72it/s]\u001b[A\n",
      " 94%|█████████▍| 734/782 [00:35<00:06,  7.69it/s]\u001b[A\n",
      " 94%|█████████▍| 735/782 [00:35<00:06,  7.66it/s]\u001b[A\n",
      " 94%|█████████▍| 736/782 [00:35<00:06,  7.60it/s]\u001b[A\n",
      " 94%|█████████▍| 737/782 [00:35<00:05,  7.53it/s]\u001b[A\n",
      " 94%|█████████▍| 738/782 [00:35<00:05,  7.45it/s]\u001b[A\n",
      " 95%|█████████▍| 739/782 [00:36<00:05,  7.38it/s]\u001b[A\n",
      " 95%|█████████▍| 740/782 [00:36<00:05,  7.33it/s]\u001b[A\n",
      " 95%|█████████▍| 741/782 [00:36<00:05,  7.23it/s]\u001b[A\n",
      " 95%|█████████▍| 742/782 [00:36<00:05,  7.17it/s]\u001b[A\n",
      " 95%|█████████▌| 743/782 [00:36<00:05,  7.08it/s]\u001b[A\n",
      " 95%|█████████▌| 744/782 [00:36<00:05,  7.04it/s]\u001b[A\n",
      " 95%|█████████▌| 745/782 [00:36<00:05,  7.01it/s]\u001b[A\n",
      " 95%|█████████▌| 746/782 [00:37<00:05,  6.95it/s]\u001b[A\n",
      " 96%|█████████▌| 747/782 [00:37<00:05,  6.89it/s]\u001b[A\n",
      " 96%|█████████▌| 748/782 [00:37<00:05,  6.79it/s]\u001b[A\n",
      " 96%|█████████▌| 749/782 [00:37<00:04,  6.74it/s]\u001b[A\n",
      " 96%|█████████▌| 750/782 [00:37<00:04,  6.66it/s]\u001b[A\n",
      " 96%|█████████▌| 751/782 [00:37<00:04,  6.63it/s]\u001b[A\n",
      " 96%|█████████▌| 752/782 [00:37<00:04,  6.53it/s]\u001b[A\n",
      " 96%|█████████▋| 753/782 [00:38<00:04,  6.47it/s]\u001b[A\n",
      " 96%|█████████▋| 754/782 [00:38<00:04,  6.43it/s]\u001b[A\n",
      " 97%|█████████▋| 755/782 [00:38<00:04,  6.42it/s]\u001b[A\n",
      " 97%|█████████▋| 756/782 [00:38<00:04,  6.36it/s]\u001b[A\n",
      " 97%|█████████▋| 757/782 [00:38<00:04,  6.25it/s]\u001b[A\n",
      " 97%|█████████▋| 758/782 [00:38<00:03,  6.18it/s]\u001b[A\n",
      " 97%|█████████▋| 759/782 [00:39<00:03,  6.08it/s]\u001b[A\n",
      " 97%|█████████▋| 760/782 [00:39<00:03,  6.02it/s]\u001b[A\n",
      " 97%|█████████▋| 761/782 [00:39<00:03,  5.93it/s]\u001b[A\n",
      " 97%|█████████▋| 762/782 [00:39<00:03,  5.85it/s]\u001b[A\n",
      " 98%|█████████▊| 763/782 [00:39<00:03,  5.79it/s]\u001b[A\n",
      " 98%|█████████▊| 764/782 [00:40<00:03,  5.70it/s]\u001b[A\n",
      " 98%|█████████▊| 765/782 [00:40<00:03,  5.66it/s]\u001b[A\n",
      " 98%|█████████▊| 766/782 [00:40<00:02,  5.57it/s]\u001b[A\n",
      " 98%|█████████▊| 767/782 [00:40<00:02,  5.47it/s]\u001b[A\n",
      " 98%|█████████▊| 768/782 [00:40<00:02,  5.35it/s]\u001b[A\n",
      " 98%|█████████▊| 769/782 [00:40<00:02,  5.29it/s]\u001b[A\n",
      " 98%|█████████▊| 770/782 [00:41<00:02,  5.23it/s]\u001b[A\n",
      " 99%|█████████▊| 771/782 [00:41<00:02,  5.11it/s]\u001b[A\n",
      " 99%|█████████▊| 772/782 [00:41<00:01,  5.01it/s]\u001b[A\n",
      " 99%|█████████▉| 773/782 [00:41<00:01,  4.93it/s]\u001b[A\n",
      " 99%|█████████▉| 774/782 [00:41<00:01,  4.84it/s]\u001b[A\n",
      " 99%|█████████▉| 775/782 [00:42<00:01,  4.77it/s]\u001b[A\n",
      " 99%|█████████▉| 776/782 [00:42<00:01,  4.67it/s]\u001b[A\n",
      " 99%|█████████▉| 777/782 [00:42<00:01,  4.59it/s]\u001b[A\n",
      " 99%|█████████▉| 778/782 [00:42<00:00,  4.51it/s]\u001b[A\n",
      "100%|█████████▉| 779/782 [00:43<00:00,  4.44it/s]\u001b[A\n",
      "100%|█████████▉| 780/782 [00:43<00:00,  4.36it/s]\u001b[A\n",
      "100%|█████████▉| 781/782 [00:43<00:00,  4.24it/s]\u001b[A\n",
      "100%|██████████| 782/782 [00:43<00:00,  4.08it/s]\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2510939727434912, tensor(0.9006, device='cuda:1'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(candidate, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in tokenizer_de(candidate)] + ['<eos>'], \n",
    "                       rotation=45)\n",
    "    ax.set_yticklabels([''] + translation)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "exm_idx = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: er ist ein guter Mann\n",
      "trg: he is a good man .\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"er ist ein guter Mann\"# \" \".join(vars(train_data.examples[exm_idx])[\"src\"])\n",
    "truth = \"he is a good man .\" # \" \".join(vars(train_data.examples[exm_idx])[\"trg\"])\n",
    "\n",
    "print(f\"src: {input_sentence}\")\n",
    "print(f\"trg: {truth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = <sos> he is a man man .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation, attention = german2english(input_sentence, SRC.vocab, model, device)\n",
    "\n",
    "print(f'predicted trg = {\" \".join(translation)}')\n",
    "\n",
    "display_attention(input_sentence, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 1\n",
    "embedding_dim = 7\n",
    "sent_len = 5\n",
    "k_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(in_channels=embedding_dim, out_channels=10, kernel_size=k_size, padding=(k_size - 1) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(batch_size, embedding_dim, sent_len)\n",
    "output = m(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.ones((1, 10))\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(k != 0.7721).permute(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7721, 0.8888, 0.9195, 0.6260, 0.4997, 0.6248, 0.9466, 0.9943, 0.1042,\n",
       "         0.7080]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = torch.rand((1, 10))\n",
    "print(attention.shape)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7721, 0.8888, 0.9195, 0.6260, 0.4997, 0.6248, 0.9466, 0.9943, 0.1042,\n",
       "         0.7080]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.masked_fill(k == 0, -1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(k == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
