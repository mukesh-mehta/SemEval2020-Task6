{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "'O',\n",
    "'I-Definition',\n",
    "'I-Term',\n",
    "'I-Secondary-Definition',\n",
    "'B-Term',\n",
    "'B-Definition',\n",
    "'I-Definiti-frag',\n",
    "'I-Qualifier',\n",
    "'I-Alias-Term',\n",
    "'B-Alias-Term',\n",
    "'B-Secondary-Definition',\n",
    "'I-Referential-Definition',\n",
    "'B-Referential-Definition',\n",
    "'B-Qualifier',\n",
    "'B-Referential-Term',\n",
    "'I-Referential-Term',\n",
    "'B-Definiti-frag',\n",
    "'I-Ordered-Definition',\n",
    "'I-Ordered-Term',\n",
    "'B-Te-frag',\n",
    "'B-Ordered-Definition',\n",
    "'B-Ordered-Term',\n",
    "'I-Te-frag',\n",
    "'B-Alias-Te-frag',\n",
    "'I-Alias-Te-frag'\n",
    "]\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "pad_token_label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WEIGHTS_NAME, BertConfig, BertForTokenClassification, BertTokenizer\n",
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertForTokenClassification, BertTokenizer),\n",
    "}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "tokenizer = tokenizer_class.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # [tokenizer.tokenize(ex) for ex in train_examples[0].words]\n",
    "# tokens = []\n",
    "# label_ids = []\n",
    "# label_map = {label: i for i, label in enumerate(label_list)}\n",
    "# example = train_examples[105]\n",
    "# for word, label in zip(example.words, example.labels):\n",
    "#     word_tokens = tokenizer.tokenize(word)\n",
    "#     tokens.extend(word_tokens)\n",
    "#     # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "#     if label[0]=='O' or label[0]=='I':\n",
    "#         print([label]+[label]*(len(word_tokens) - 1))\n",
    "#         label_ids.extend([label_map[label]]+[label_map[label]]*(len(word_tokens) - 1))\n",
    "#     elif label[0]=='B':\n",
    "#         print([label]+[\"I\"+label[1:]]*(len(word_tokens) - 1))\n",
    "#         label_ids.extend([label_map[label]]+[label_map[\"I\"+label[1:]]]*(len(word_tokens) - 1))\n",
    "# label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples,\n",
    "                                 label_list,\n",
    "                                 max_seq_length,\n",
    "                                 tokenizer,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 cls_token=\"[CLS]\",\n",
    "                                 cls_token_segment_id=1,\n",
    "                                 sep_token=\"[SEP]\",\n",
    "                                 sep_token_extra=False,\n",
    "                                 pad_on_left=False,\n",
    "                                 pad_token=0,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 pad_token_label_id=pad_token_label_id,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "#         if ex_index % 10000 == 0:\n",
    "#             logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            if label[0]=='O' or label[0]=='I':\n",
    "#                 print([label_map[label]]+[label_map[label]]*(len(word_tokens) - 1))\n",
    "                label_ids.extend([label_map[label]]+[label_map[label]]*(len(word_tokens) - 1))\n",
    "            elif label[0]=='B':\n",
    "#                 print([label_map[label]]+[label_map[\"I\"+label[1:]]]*(len(word_tokens) - 1))\n",
    "                label_ids.extend([label_map[label]]+[label_map[\"I\"+label[1:]]]*(len(word_tokens) - 1))\n",
    "                \n",
    "        \n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = 3 if sep_token_extra else 2\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[:(max_seq_length - special_tokens_count)]\n",
    "\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "        else:\n",
    "            input_ids += ([pad_token] * padding_length)\n",
    "            input_mask += ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            segment_ids += ([pad_token_segment_id] * padding_length)\n",
    "            label_ids += ([pad_token_label_id] * padding_length)\n",
    "        \n",
    "#         print(len(label_ids), max_seq_length)\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "#         try:\n",
    "        assert len(label_ids) == max_seq_length\n",
    "#         except:\n",
    "#             print(example.words, example.labels)\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=label_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"max_seq_length\": 100,\n",
    "    \"model_type\": \"bert\",\n",
    "    \"num_train_epochs\":1,\n",
    "    \"gradient_accumulation_steps\":1,\n",
    "    'learning_rate':5e-5,\n",
    "    'adam_epsilon':1e-8,\n",
    "    'warmup_steps':0,\n",
    "    'weight_decay':0.0,\n",
    "    'max_grad_norm':1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(examples, tokenizer, labels, pad_token_label_id):\n",
    "    features = convert_examples_to_features(examples, labels, config['max_seq_length'], tokenizer,\n",
    "                                            cls_token_at_end=bool(config['model_type'] in [\"xlnet\"]),\n",
    "                                            # xlnet has a cls token at the end\n",
    "                                            cls_token=tokenizer.cls_token,\n",
    "                                            cls_token_segment_id=2 if config['model_type'] in [\"xlnet\"] else 0,\n",
    "                                            sep_token=tokenizer.sep_token,\n",
    "                                            sep_token_extra=bool(config['model_type'] in [\"roberta\"]),\n",
    "                                            # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "                                            pad_on_left=bool(config['model_type'] in [\"xlnet\"]),\n",
    "                                            # pad on the left for xlnet\n",
    "                                            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                            pad_token_segment_id=4 if config['model_type'] in [\"xlnet\"] else 0,\n",
    "                                            pad_token_label_id=pad_token_label_id\n",
    "                                            )\n",
    "#         if args.local_rank in [-1, 0]:\n",
    "#             logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "#             torch.save(features, cached_features_file)\n",
    "\n",
    "#     if args.local_rank == 0 and not evaluate:\n",
    "#         torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\",sep = \",\")\n",
    "train_df.dropna(inplace=True)\n",
    "train_examples = [InputExample(i, str(text).split(\" \"), str(label).split(\" \")) for i, (text, label) in enumerate(zip(train_df['text'].values, train_df['labels'].values))]\n",
    "train_dataset = load_and_cache_examples(train_examples, tokenizer, label_list, pad_token_label_id)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=8)\n",
    "t_total = len(train_dataloader) // config['gradient_accumulation_steps'] * config['num_train_epochs']\n",
    "t_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"val.csv\",sep = \",\")\n",
    "eval_df.dropna(inplace=True)\n",
    "eval_examples = [InputExample(i, str(text).split(\" \"), str(label).split(\" \")) for i, (text, label) in enumerate(zip(eval_df['text'].values, eval_df['labels'].values))]\n",
    "eval_dataset = load_and_cache_examples(eval_examples, tokenizer, label_list, pad_token_label_id)\n",
    "eval_sampler = RandomSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "config_ = config_class.from_pretrained('bert-base-uncased',\n",
    "                                          num_labels=len(label_list))\n",
    "model = model_class.from_pretrained('bert-base-uncased', config=config_)\n",
    "model.to('cuda')\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": config['weight_decay']},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'], eps=config['adam_epsilon'])\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=config['warmup_steps'], t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_model(model, train_dataloader):\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    device='cuda'\n",
    "    # train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    # set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n",
    "    for _ in range(config['num_train_epochs']):\n",
    "    #     epoch_iterator = tqdm(train_dataloader)\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                      \"attention_mask\": batch[1],\n",
    "                      \"token_type_ids\": batch[2] if config['model_type'] in [\"bert\", \"xlnet\"] else None,\n",
    "                      # XLM and RoBERTa don\"t use segment_ids\n",
    "                      \"labels\": batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            if config['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / config['gradient_accumulation_steps']\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "    print(global_step, tr_loss / global_step)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate(model, eval_dataloader, labels):\n",
    "    device = \"cuda\"\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                      \"attention_mask\": batch[1],\n",
    "                      \"token_type_ids\": batch[2] if config['model_type'] in [\"bert\", \"xlnet\"] else None,\n",
    "                      # XLM and RoBERTa don\"t use segment_ids\n",
    "                      \"labels\": batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "    for i in range(out_label_ids.shape[0]):\n",
    "        for j in range(out_label_ids.shape[1]):\n",
    "            if out_label_ids[i, j] != pad_token_label_id:\n",
    "                out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    results = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list)\n",
    "    }\n",
    "    return results, preds_list, out_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=train_model(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, preds_list, out_label_list = evaluate(model, eval_dataloader, label_list)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
