{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        \n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"../deft_corpus/data/Task1_folds/train_0.csv\", sep=\"\\t\")[['text','has_def']]\n",
    "train_examples = [InputExample(i, text, None, label) for i, (text, label) in enumerate(zip(train_df.iloc[:, 0], train_df.iloc[:, 1]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = self.load_and_cache_examples(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.InputExample at 0x7f388167cad0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForSequenceClassification, BertTokenizer,\n",
    "                                  RobertaConfig,\n",
    "                                  RobertaForSequenceClassification,\n",
    "                                  RobertaTokenizer,\n",
    "                                  XLMConfig, XLMForSequenceClassification,\n",
    "                                  XLMTokenizer, XLNetConfig,\n",
    "                                  XLNetForSequenceClassification,\n",
    "                                  XLNetTokenizer,\n",
    "                                  DistilBertConfig,\n",
    "                                  DistilBertForSequenceClassification,\n",
    "                                  DistilBertTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "            'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "            'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "            'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "            'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "            'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),\n",
    "        }\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['roberta']\n",
    "tokenizer = tokenizer_class.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            'output_dir': 'outputs/',\n",
    "            'cache_dir': 'cache_dir',\n",
    "            'max_seq_length': 100,\n",
    "            'train_batch_size': 8,\n",
    "            'gradient_accumulation_steps': 1,\n",
    "            'eval_batch_size': 8,\n",
    "            'num_train_epochs': 4,\n",
    "            'weight_decay': 0,\n",
    "            'learning_rate': 4e-5,\n",
    "            'adam_epsilon': 1e-8,\n",
    "            'warmup_ratio': 0.06,\n",
    "            'warmup_steps': 0,\n",
    "            'max_grad_norm': 1.0,\n",
    "\n",
    "            'logging_steps': 50,\n",
    "            'save_steps': 1000,\n",
    "\n",
    "            'overwrite_output_dir': False,\n",
    "            'reprocess_input_data': False,\n",
    "            'label_list':[0,1]\n",
    "        }\n",
    "\n",
    "config['model_name'] = 'bert-base-uncased'\n",
    "config['model_type'] = 'bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(examples, tokenizer, evaluate=False, no_cache=False):\n",
    "    \"\"\"\n",
    "    Converts a list of InputExample objects to a TensorDataset containing InputFeatures. Caches the InputFeatures.\n",
    "\n",
    "    Utility function for train() and eval() methods. Not intended to be used directly.\n",
    "    \"\"\"\n",
    "\n",
    "#     process_count = self.args['process_count']\n",
    "\n",
    "    tokenizer = tokenizer\n",
    "    output_mode = 'classification'\n",
    "#     args=self.args\n",
    "\n",
    "    if not os.path.isdir(config['cache_dir']):\n",
    "        os.mkdir(config['cache_dir'])\n",
    "\n",
    "    mode = 'dev' if evaluate else 'train'\n",
    "    cached_features_file = os.path.join(config['cache_dir'], f\"cached_{mode}_{config['model_type']}_{config['max_seq_length']}_binary\")\n",
    "\n",
    "    features = convert_examples_to_features(examples,\n",
    "                                    tokenizer,\n",
    "                                    label_list=config['label_list'],\n",
    "                                    max_length=config['max_seq_length'],\n",
    "                                    output_mode=output_mode,\n",
    "                                    pad_on_left=bool(config['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n",
    "                                    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                    pad_token_segment_id=4 if config['model_type'] in ['xlnet'] else 0,\n",
    "    )\n",
    "\n",
    "#         if not no_cache:\n",
    "#             torch.save(features, cached_features_file)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "#     all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "\n",
    "train_dataset = load_and_cache_examples(train_examples, tokenizer, evaluate=False, no_cache=False)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=config['train_batch_size'])\n",
    "t_total = len(train_dataloader) // config['gradient_accumulation_steps'] * config['num_train_epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "model = model_class.from_pretrained('roberta-base', num_labels=2)\n",
    "model.to('cuda')\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': config['weight_decay']},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "warmup_steps = math.ceil(t_total * config['warmup_ratio'])\n",
    "config['warmup_steps'] = warmup_steps if config['warmup_steps'] == 0 else config['warmup_steps']\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'], eps=config['adam_epsilon'])\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=config['warmup_steps'], t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_model(model, train_dataloader):\n",
    "    device = 'cuda'\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    for _ in range(int(config['num_train_epochs'])):\n",
    "        # epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Current iteration\")):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'labels':         batch[3]}\n",
    "            # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
    "            if config['model_type'] != 'distilbert':\n",
    "                inputs['token_type_ids'] = batch[2] if config['model_type'] in ['bert', 'xlnet'] else None  \n",
    "            outputs = model(**inputs)\n",
    "            # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss = outputs[0]\n",
    "            if config['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / config['gradient_accumulation_steps']\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if config['save_steps'] > 0 and global_step % config['save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(\n",
    "                        config['output_dir'], 'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Take care of distributed/parallel training\n",
    "                    model_to_save = model.module if hasattr(\n",
    "                        model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "        print(\"Epoch \", _, global_step, tr_loss / global_step)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "eval_df = pd.read_csv(\"../deft_corpus/data/Task1_folds/task1_dev.csv\", sep=\"\\t\")[['text','has_def']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = [InputExample(i, text, None, label) for i, (text, label) in enumerate(zip(eval_df.iloc[:, 0], eval_df.iloc[:, 1]))]\n",
    "eval_dataset = load_and_cache_examples(eval_examples, tokenizer, evaluate=False, no_cache=False)\n",
    "eval_sampler = RandomSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=config['eval_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "def evaluate(model, eval_dataloader):\n",
    "    output_mode = \"classification\"\n",
    "    device = \"cuda\"\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'labels':         batch[3]}\n",
    "            # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
    "#             preds.extend(batch[3])\n",
    "            if config['model_type'] != 'distilbert':\n",
    "                inputs['token_type_ids'] = batch[2] if config['model_type'] in ['bert', 'xlnet'] else None  \n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if output_mode == \"classification\":\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "    elif output_mode == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    return preds, out_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current iteration: 100%|██████████| 1848/1848 [15:22<00:00,  2.00it/s]\n",
      "Current iteration:   0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 1848 0.4904240579324174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current iteration: 100%|██████████| 1848/1848 [16:12<00:00,  1.90it/s]\n",
      "Current iteration:   0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 3696 0.43724934169236424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current iteration: 100%|██████████| 1848/1848 [16:01<00:00,  1.92it/s]\n",
      "Current iteration:   0%|          | 0/1848 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 5544 0.3938392501648295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current iteration: 100%|██████████| 1848/1848 [15:42<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 7392 0.3518529035968936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:11<00:00,  8.89it/s]\n"
     ]
    }
   ],
   "source": [
    "preds, out_label_ids = evaluate(model, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89       534\n",
      "           1       0.79      0.82      0.80       277\n",
      "\n",
      "    accuracy                           0.86       811\n",
      "   macro avg       0.85      0.85      0.85       811\n",
      "weighted avg       0.86      0.86      0.86       811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(out_label_ids, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
