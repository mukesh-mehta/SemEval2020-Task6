{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import logging as log\n",
    "\n",
    "train = pd.read_csv(\"../deft_corpus/data/Task1_folds/train_0.csv\", sep=\"\\t\")\n",
    "val = pd.read_csv(\"../deft_corpus/data/Task1_folds/val_0.csv\", sep=\"\\t\")\n",
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['has_def'].sum() + val['has_def'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(train['filename'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../deft_corpus/data/task1_dev.csv\", sep=\"\\t\")\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(text): \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits)).strip()\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "train['preprocessed'] = train['text'].map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=tokenizer, lower=True, use_vocab=True, pad_token=\"<PAD>\")\n",
    "\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "tv_datafields = [(\"text\", TEXT), (\"has_def\", LABEL),\n",
    "                 (\"filename\", None)]\n",
    "trn, vld = TabularDataset.splits(\n",
    "               path=\"../deft_corpus/data/Task1_folds/\", # the root directory where the data lies\n",
    "               train='train_0.csv',validation=\"val_0.csv\",\n",
    "               format='tsv',\n",
    "               skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=tv_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trn[65].text),len(trn[1].text),len(trn[48].text),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if len(trn[i].text)<10:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchtext.vocab as vocab\n",
    "# import os\n",
    "# TEXT.build_vocab(trn, vld)\n",
    "# vectors = vocab.Vectors(os.path.join(emb_path, \"glove.6B.100d.txt\"), cache=emb_path)\n",
    "# TEXT.vocab.set_vectors(vectors.stoi, vectors.vectors, vectors.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn, vld, vectors=\"glove.6B.100d\", max_size=20000,\n",
    "        min_freq=50)#TEXT.build_vocab(train, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "import torch\n",
    "train_iter, val_iter = BucketIterator.splits(\n",
    " (trn, vld), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(128, 128),\n",
    " device=torch.device('cpu'), # if you want to use the GPU, specify the GPU number here\n",
    " sort_key=lambda x: len(x.text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    " sort_within_batch=False,\n",
    " repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x \n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "train_dl = BatchWrapper(train_iter, 'text', ['has_def'])\n",
    "valid_dl = BatchWrapper(val_iter, 'text', ['has_def'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(train_dl.__iter__())\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class SimpleBiLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=100,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=2):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(TEXT.vocab.vectors))\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 100\n",
    "nh = 100\n",
    "nl = 3\n",
    "model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz).to('cpu'); model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = torch.FloatTensor([compute_class_weight('balanced', [0,1], train['has_def'].values)[1]])\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log.basicConfig(\n",
    "#     filename = 'trainlog.txt', \n",
    "#     format   = '%(asctime)s : %(message)s',\n",
    "#     datefmt  = '%Y-%m-%d %H:%M:%S', \n",
    "#     level = log.INFO)\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\"log_{}.log\".format(timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(\"log_{}.log\".format(timestr),\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import sys\n",
    "old_stdout = sys.stdout\n",
    "\n",
    "sys.stdout = log_file\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    train_preds = []\n",
    "    train_truth = []\n",
    "    for x, y in tqdm.tqdm(train_dl):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_preds.extend(nn.Sigmoid()(preds).detach().cpu().numpy())\n",
    "        train_truth.extend(y.cpu().numpy())\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "\n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_truth = []\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "        val_preds.extend(nn.Sigmoid()(preds).detach().cpu().numpy())\n",
    "        val_truth.extend(y.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))\n",
    "    print(\"classification report Train\")\n",
    "    train_preds = np.where(np.array(train_preds)<0.5, 0, 1).flatten()\n",
    "    print(classification_report(train_truth, train_preds))\n",
    "    \n",
    "    print(\"classification report Validation\")\n",
    "    val_preds = np.where(np.array(val_preds)<0.5, 0, 1).flatten()\n",
    "    print(classification_report(val_truth, val_preds))\n",
    "sys.stdout = old_stdout\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
